{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMBD Review Binary Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNb9HQ0mZZ2F909dD3UB3Lx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrtzcardo/IMBD-Review-Binary-Classification/blob/main/IMBD_Review_Binary_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxJmpDGfPnN9"
      },
      "source": [
        "'''IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
        "Internet Movie Database.\n",
        "Each set consisting of 50% negative and 50% positive reviews.\n",
        "The reviews (sequences of words) have been turned into\n",
        "sequences of integers, where each integer stands for a specific word in a dictionary.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BCP0QkJSRIZ"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTAMP5BNSqyb"
      },
      "source": [
        "Pad  lists so that they all have the same length, turn them into an integer\n",
        "tensor of shape (samples, word_indices), and then use as the first layer in\n",
        "your network a layer capable of handling such integer tensors (the Embedding\n",
        "layer).\n",
        "\n",
        "One-hot encode  lists to turn them into vectors of 0s and 1s. This would\n",
        "mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector\n",
        "that would be all 0s except for indices 3 and 5, which would be 1s. Then it could be used as the first layer in the network a Dense layer, capable of handling floating-point vector data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx_9q7S1SY9N"
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))  #Creates an all-zero matrix of shape (len(sequences),dimension)\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.   #Sets specific indices of results[i] to 1s\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h69LAmhzMzj6"
      },
      "source": [
        "\n",
        "'''The argument num_words=10000 means only keeping the top 10,000 most frequently\n",
        "occurring words in the training data. Rare words will be discarded. This allows\n",
        "working with vector data of manageable size.'''\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX9k-bH-M5OR"
      },
      "source": [
        "print(train_data[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swFvBQwXNNru"
      },
      "source": [
        "'''Restricting to the top 10,000 most frequent words, no word\n",
        "index will exceed 10,000:'''\n",
        "\n",
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLJB_DN4OG9X"
      },
      "source": [
        "'''For kicks, let's quickly decode one of these reviews back to English\n",
        "words:'''\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "[(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join(\n",
        "[reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqCbGDpNOHyF"
      },
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ph61GZTDtQ"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBsZ5qqTxK8"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fec2J-UAVhPx"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "                        loss='mse', #loss='binary_crossentropy'\n",
        "                        metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra6p4hWLW5mE"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmgyfeWaXegO"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m93aaAKTX1hD"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "print(history_dict.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02o-O3LYqze"
      },
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, 21)               #epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB6xGpjgZQv_"
      },
      "source": [
        "plt.clf()\n",
        "acc_values = history_dict['acc']   #history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_acc']   #history_dict['val_accuracy']\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')  \n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGtmqHYqagnW"
      },
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai8iflmqcDAn"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIpKnQKXcKrM"
      },
      "source": [
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxOaL5SkcW6-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}